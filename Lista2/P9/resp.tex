% Solamente para hacer la versión móbil
\documentclass[a5paper]{article}
\usepackage[margin=5mm]{geometry}
\usepackage{tgheros}
\usepackage[T1]{fontenc}
\renewcommand*\familydefault{\sfdefault}
%%%%%%%%%%%%%%%%%%%%%



% \documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage[svgnames]{xcolor}
\usepackage{blkarray}
\usepackage{breqn}
\usepackage{caption}


%opening
\title{9. Simplifcació de la barreja de Gaussianes 1 [G]}
\author{
Josep de Cid
\and Albert Ribes
\and Kerstin Winter
}

\begin{document}

\maketitle

% \begin{abstract}
%
% \end{abstract}

% \section{}

\textbf{
Considereu el model de barreja de Gaussianes:
}

\begin{equation*}
p(x) = \sum_{k = 1}^{K} \pi_k \mathcal{N}(x; \mu_k, \Sigma_k)
\end{equation*}

\textbf{
Preneu el cas que totes les matrius de covariança són iguals i diagonals, és a dir, $\Sigma_1 = \dots, \Sigma_K = \Sigma = diag(\sigma_1^2,\dots, \sigma_d^2)$
}



\begin{enumerate}
\item \textbf{Enraoneu en quin sentit representa una simplifcació respecte al cas general (amb matrius de
covariança generals), des dels punts de vista estadístic i geomètric.
}

Significa que las dimensiones son independientes entre ellas.

Geométricamente esto significa que cada uno de los clusters generará instancias formando una probabilidad con forma elipsoide alargada en la cual el eje mayor es paralelo a alguno de los ejes del sistema de coordenadas.
\item \textbf{Expresseu la funció de densitat de probabilitat $\mathcal{N}(x; \mu_k, \Sigma_k)$ que en resulta.
}

En el caso genérico la función de densidad de la distribución gaussiana multivariada con $D$ variables es:

\begin{equation*}
\mathcal{N}(x;\mu,\Sigma) = \frac{1}{(2\pi)^{(\frac{D}{2})}} \cdot \frac{1}{(|\Sigma|)^{\frac{1}{2}}} exp\Bigg( -\frac{1}{2} (x - \mu)^T\Sigma^{-1}(x - \mu) \Bigg)
\end{equation*}

Pero se puede simplificar con las siguientes propiedades:
\begin{itemize}
\item $det( diag(\alpha_1,\alpha_2, \dots, \alpha_n) ) = \prod_{i = 1}^n \alpha_i$
\item $diag(\alpha_1, \alpha_2, \dots, \alpha_n)^{-1} = diag(\alpha_1^{-1}, \alpha_2^{-1}, \dots, \alpha_n^{-1})$
\item $(\alpha_1,\alpha_2, \dots, \alpha_n)^T \cdot diag(\beta_1,\beta_2, \dots, beta_n) = (\alpha_1\beta_1, \alpha_2\beta_2, \dots, \alpha_n\beta_n)^T$
\end{itemize}

Y entonces la fórmula anterior queda:

\begin{equation*}
\mathcal{N}(x;\mu,\Sigma) = \frac{1}{(2\pi)^{(\frac{D}{2})}} \cdot \frac{1}{(\prod_{i = 1}^n \sigma_i)} exp\Bigg( -\frac{1}{2} \sum_{i = 1}^{d} \Big( \frac{(x_i - \mu_i)^2}{\sigma_i^2} \Big) \Bigg)
\end{equation*}
\item \textbf{Construïu la funció de log-versemblança negativa.
}

En el caso de que se disponga de $M$ datos $D = \{x_1, x_2, \dots, x_M\}$ la función log-verosimilitud negativa es:
\begin{equation*}
l(\pi, \mu, \Sigma; D) = -\ln \prod_{j = 1}^{M} p(x_j) 
\end{equation*}

\begin{equation*}
l(\pi, \mu, \Sigma; D) = -\sum_{j = 1}^{M} \ln(p(x_j)) 
\end{equation*}

\begin{equation*}
l(\pi, \mu, \Sigma; D) = -\sum_{j = 1}^{M} \Bigg[ 
\ln\Big(
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Big)
\Bigg] 
\end{equation*}


% \begin{equation*}
% l = -\sum_{j = 1}^{M} \Bigg[ 
% \ln\Big(
% \sum_{k = 1}^{K}
% \big(
%     \pi_k \frac{1}{(2\pi)^{(\frac{D}{2})}} \cdot \frac{1}{(\prod_{i = 1}^n \sigma_i)} exp\Bigg( -\frac{1}{2} \sum_{i = 1}^{d} \Big( \frac{(x_{ji} - \mu_i)^2}{\sigma_i^2} \Big) \Bigg)
% \big)
% \Big)
% \Bigg] 
% \end{equation*}

\item \textbf{Deriveu les equacions de l'algorisme E-M que en resulta i escriviu l'algorisme de clustering complet.
}

Antes de seguir avanzando vamos a definir la función $\gamma_k(x)$, que es la probabilidad de que sea la distribución $k$ la que ha generado el dato $x$. Formalmente:

\begin{equation*}
\gamma_k(x) = p(z_k = 1 | x)
\end{equation*}

\begin{equation*}
\gamma_k(x) =
\frac
{p(x_k = 1)p(x | z_k = 1)}
{\sum_{j = 1}^{K} p(z_j = 1)p(x | z_j = 1)}
\end{equation*}

\begin{equation*}
\gamma_k(x) =
\frac
{\pi_k \mathcal{N}(x; \mu_k, \Sigma_k)}
{\sum_{j = 1}^{K}
    \pi_j \mathcal{N}(x; \mu_j, \Sigma_j)
}
\end{equation*}

Esta función nos será útil más adelante.

Para encontrar los extremos vamos a tener que derivar respecto a los valores de $\pi$, $\mu$ y $\Sigma$, y además debe cumplirse la condición de que $\sum_{k = 1}^{K} \pi_k = 1$. Para resolver este problema usaremos los multiplicadores de Lagrange.

La función de Lagrange es:

\begin{equation*}
\mathcal{L}(\pi, \mu, \Sigma, \lambda) = l(\pi, \mu, \Sigma; D) - \lambda \cdot \Big[(\sum_{k = 1}^{K} \pi_k) - 1\Big]
\end{equation*}

\begin{equation*}
\mathcal{L}(\pi, \mu, \Sigma, \lambda) =
-\sum_{j = 1}^{M} \Bigg[ 
\ln\Big(
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Big)
\Bigg] 
- \lambda \cdot \Big[(\sum_{k = 1}^{K} \pi_k) - 1\Big]
\end{equation*}

Como más adelante hará falta, calculamos previamente algunas derivadas intermedias:

\begin{equation*}
\frac{\partial \mathcal{N}(x; \mu,\Sigma)}{\partial \mu_q} = 
\mathcal{N}(x; \mu, \Sigma) \cdot
\Big[
    -\frac{1}{2} \cdot \frac{2(x_q - \mu_q)}{\sigma_q^2} (-1)
\Big]
\end{equation*}

\begin{equation}
\label{eq:1}
\frac{\partial \mathcal{N}(x; \mu,\Sigma)}{\partial \mu_q} = 
\mathcal{N}(x; \mu, \Sigma)
    \frac{(x_q - \mu_q)}{\sigma_q^2}
\end{equation}

Hay que derivar respecto a los valores de $\pi$, $\mu$ y $\Sigma$ y $\lambda$ e igualar a 0.

\begin{equation*}
\frac
{\partial \mathcal{L}(\pi,\mu, \Sigma, \lambda)}
{\partial \pi_q} = 
\frac
{\partial l(\pi, \mu, \Sigma; D)}
{\partial \pi_q}
-
\frac
{\partial \lambda
\Big[
    \Big(
        \sum_{k = 1}^{K} \pi_k
    \Big) - 1
\Big]
}
{\partial \pi_q}
\end{equation*}

Y si lo hacemos por partes:

\begin{equation*}
\frac
{\partial l(\pi, \mu, \Sigma; D)}
{\partial \pi_q} = 
-\sum_{j = 1}^{M} \Bigg[ 
\frac{1}{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot \frac{\partial \Big( 
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Big)}{\partial \pi_q}
\Bigg] 
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \pi_q} = 
-\sum_{j = 1}^{M} \Bigg[ 
\frac{1}{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot \mathcal{N}(x_j, \mu_q, \Sigma_q)
\Bigg] 
\end{equation*}


\begin{equation*}
\frac{\partial l}{\partial \pi_q} = 
-\sum_{j = 1}^{M} \Bigg[ 
\frac
{\mathcal{N}(x_j, \mu_q, \Sigma_q)}
{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\Bigg] 
\end{equation*}

Y esto se puede expresar usando la función $\gamma_k$ que hemos definido previamente:

\begin{equation*}
\frac
{\partial l(\pi, \mu, \Sigma; D)}
{\partial \pi_q} = 
-\sum_{j = 1}^{M}
    \Bigg[
        \frac
        {\gamma_q(x_j)}
        {\pi_q}
    \Bigg]
\end{equation*}

Y
\begin{equation*}
\frac
{\partial \lambda
\Big[
    \Big(
        \sum_{k = 1}^{K} \pi_k
    \Big) - 1
\Big]
}
{\partial \pi_q} = 
\lambda
\end{equation*}

Y por lo tanto:
\begin{equation*}
\frac
{\partial \mathcal{L}(\pi,\mu, \Sigma, \lambda)}
{\partial \pi_q} = 
-\sum_{j = 1}^{M}
    \Bigg[
        \frac
        {\gamma_q(x_j)}
        {\pi_q}
    \Bigg]
- \lambda
\end{equation*}

% Por simplificar las equaciones derivamos por separado $\mathcal{N}$:
% 
% \begin{equation*}
% \mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{(\frac{D}{2})}} \cdot \frac{1}{(\prod_{i = 1}^n \sigma_i)} exp\Bigg( -\frac{1}{2} \sum_{i = 1}^{d} \frac{(x_i - \mu_i)^2}{\sigma_i^2} \Bigg)
% \end{equation*}
% 
% 
% \begin{equation*}
% \frac{\partial \mathcal{N}(x|\mu, \Sigma)}{\partial \mu_q} = \mathcal{N}
% \cdot \frac
% {\partial \Big(
% -\frac{1}{2} \sum_{i = 1}^{d} \frac{(x_i - \mu_i)^2}{\sigma_i^2}
% \Big)}
% {\partial \mu_q}
% \end{equation*}
% 
% \begin{equation*}
% \frac{\partial \mathcal{N}(x|\mu, \Sigma)}{\partial \mu_q} = \mathcal{N}
% \cdot
% \Big(
% -\frac{1}{2}
%     \big(
%         \frac{1}{\sigma_q^2}
%         2(x_q - \mu_q)
%         (-1)
%     \big)
% \Big)
% \end{equation*}
% 
% 
% \begin{equation*}
% \frac{\partial \mathcal{N}(x|\mu, \Sigma)}{\partial \mu_q} =
% \mathcal{N}
% \cdot
% \Big(
%         \frac{x_q - \mu_q}{\sigma_q^2}
% \Big)
% \end{equation*}



La derivada respecto a cada una de las $\mu_q$ es:

\begin{equation*}
l(\pi, \mu, \Sigma; D) = -\sum_{j = 1}^{M} \Bigg[ 
\ln\Big(
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Big)
\Bigg] 
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M} \Bigg[
\frac{1}
{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot
\Big(
    \sum_{k = 1}^{K} \pi_k
    \frac{\partial \mathcal{N}(x_j; \mu_k, \Sigma_k)}{\partial \mu_q}
\Big)
\Bigg]
\end{equation*}


Y si usamos la ecuación \ref{eq:1} tenemos:
\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M} \Bigg[
\frac{1}
{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot
\Big(
    \sum_{k = 1}^{K} \pi_k
%     \mathcal{N}
% \cdot
% \Big(
%         \frac{x_q - \mu_q}{\sigma_q^2}
% \Big)
\mathcal{N}(x_j; \mu_k, \Sigma_k)
    \frac{(x_q - \mu_q)}{\sigma_q^2}
\Big)
\Bigg]
\end{equation*}


\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M} \Bigg[
\frac{1}
{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot
\Big(
        \frac{x_q - \mu_q}{\sigma_q^2}
\Big)
    \sum_{k = 1}^{K} \pi_k
    \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Bigg]
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M} \Bigg[
\frac
{
\sum_{k = 1}^{K} \pi_k
    \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
{
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
}
\cdot
\Big(
        \frac{x_q - \mu_q}{\sigma_q^2}
\Big)
\Bigg]
\end{equation*}

Tengo que corregir a partir de aquí
% TODO
\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M} \Bigg[
\frac{1}
{
p(x_j)
}
\cdot
\Big(
        \frac{x_q - \mu_q}{\sigma_q^2}
\Big)
    p(x_j)
\Bigg]
\end{equation*}


\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
-\sum_{j = 1}^{M}
        \frac{x_q - \mu_q}{\sigma_q^2}
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
\sum_{j = 1}^{M}
        \frac{\mu_q - x_q}{\sigma_q^2}
\end{equation*}


\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
        \frac{M(\mu_q - x_q)}{\sigma_q^2}
\end{equation*}

\begin{equation*}
\frac{\partial l}{\partial \mu_q} =
        \frac{M\mu_q}{\sigma_q^2} - 
        \frac{Mx_q}{\sigma_q^2}
\end{equation*}


La derivada respecto a $\sigma_q^2$ es:

\begin{equation*}
l(\pi, \mu, \Sigma; D) = -\sum_{j = 1}^{M} \Bigg[ 
\ln\Big(
\sum_{k = 1}^{K} \pi_k \mathcal{N}(x_j; \mu_k, \Sigma_k)
\Big)
\Bigg] 
\end{equation*}

\begin{equation*}
\frac{\partial l(\pi, \mu, \Sigma; D)}{\partial \Sigma_q} = -\sum_{j = 1}^{M} 
\Bigg[ 
    \frac{}{}
\Bigg] 
\end{equation*}



\item \textbf{Enraoneu sobre les implicacions (possibles avantatges/inconvenients) que representa la simplifcació
respecte el cas general des del punt de vista del clustering.
}
\end{enumerate}
\end{document}
