\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[shortlabels]{enumitem}
\usepackage{ textcomp }

%% Una forma de poner títulos más pequeños
% \usepackage{titlesec}
% \titleformat*{\section}{\small\bfseries}
%%%%%%%%%%%%%%%%


%% Otra forma de hacerlo
\usepackage{sectsty}
\sectionfont{\small}
%%%%%%%%

%opening
\title{Problema 3: Màxima Versemblança 1}
\author{
Gerard Barrachina
\and
Josep de Cid
\and
Albert Ribes
\and
Kerstin Winter
}

\begin{document}

\maketitle

% \begin{abstract}
%
% \end{abstract}

% \section{}

Considerem un experiment aleatori en què mesurem una determinada variable aleatòria $X$,
que segueix
una distribució gaussiana univariada, cosa que escrivim $X \sim N(\mu, \sigma^2)$. Prenem $N$
mesures independents de $X$ i obtenim una mostra aleatòria simple $\{x_1, \dots, x_N\}$,
on cada $x_n$ es una realització de $X$, per $n = 1,\dots,N$. Es demana:

\section{Escriviu la funció de densitat de probabilitat per un $x_n$ qualsevol i construïu la funció log-versemblança (negativa) de la mostra.}

\begin{equation*}
f(x_j | \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_j - \mu)^2}{2\sigma^2}}
\end{equation*}

\begin{equation*}
lh = - \sum_{n = 1}^{N} \Big[ \ln{\big( f(x_n | \mu, \sigma^2)\big)} \Big]
\end{equation*}

\begin{equation*}
lh = - \sum_{n = 1}^{N} \Big[ \ln{\Big(  \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x_j - \mu)^2}{2\sigma^2}} \Big)} \Big]
\end{equation*}

\begin{equation*}
lh = - \sum_{n = 1}^{N} \Big[ \ln{\Big(  \frac{1}{\sigma\sqrt{2\pi}}\Big)} + \ln{\Big(e^{-\frac{(x_j - \mu)^2}{2\sigma^2}} \Big)} \Big]
\end{equation*}

\begin{equation*}
lh = - \sum_{n = 1}^{N} \Big[ -\ln{\big(  \sigma\sqrt{2\pi} \big)} - \frac{(x_j - \mu)^2}{2\sigma^2} \Big]
\end{equation*}

\begin{equation*}
lh = \sum_{n = 1}^{N} \Big[ \ln{\big(  \sigma\sqrt{2\pi} \big)} + \frac{(x_j - \mu)^2}{2\sigma^2} \Big]
\end{equation*}

\section{Trobeu els estimadors de màxima versemblança $\hat{\mu}$ i $\hat{\sigma^2}$ per $\mu$ i per $\sigma^2$, a partir de la mostra}

Debemos encontrar los valores de $\sigma^2$ y $\mu$ que hacen que $lh$ sea mínimo (puesto que hemos cambiado el signo a la función de densidad). Para ello su derivada debe ser $0$ y su segunda derivada en ese punto debe dar un resultado positivo.

La derivada de la función de máxima verosimilitud (likelihood) respecto de $\sigma^2$ es:

\begin{equation*}
lh = \sum_{n = 1}^{N} \Big[ \ln{\big(  \sigma\sqrt{2\pi} \big)} + \frac{(x_j - \mu)^2}{2\sigma^2} \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \sigma^2} = \sum_{n = 1}^{N} \Big[ \Big( \frac{1}{\sigma\sqrt{2\pi}} \cdot \frac{\sqrt{2\pi}}{2\sigma} \Big) + \Big( \frac{(x_j - \mu)^2}{2} \big( -\frac{1}{\sigma^4}\big)\Big)\Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \sigma^2} = \sum_{n = 1}^{N} \Big[ \frac{1}{2\sigma^2} - \frac{(x_j - \mu)^2}{2\sigma^4} \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \sigma^2} = \frac{N}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

Entonces para que $\frac{\partial lh}{\partial \sigma^2} = 0 $ se tiene que cumplir que:

\begin{equation*}
\frac{N}{2\sigma^2} = \frac{1}{2\sigma^4}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

\begin{equation*}
\frac{N2\sigma^4}{2\sigma^2} = \sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

\begin{equation*}
N\sigma^2 = \sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

\begin{equation*}
\sigma^2 = \frac{1}{N}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

% \begin{equation*}
% N\sigma^2 = \sum_{n = 1}^{N} (x_j^2 -2\mu x_j + \mu^2)
% \end{equation*}
%
% \begin{equation*}
% N\sigma^2 = \sum_{n = 1}^{N} x_j^2 -\sum_{n = 1}^{N} 2\mu x_j + \sum_{n = 1}^{N} \mu^2
% \end{equation*}
%
% \begin{equation*}
% N\sigma^2 = \sum_{n = 1}^{N} x_j^2 -N2\mu\sum_{n = 1}^{N} x_j + N\mu^2
% \end{equation*}
%
% \begin{equation*}
% \sigma^2 = \frac{1}{N}\sum_{n = 1}^{N} x_j^2 -2\mu\sum_{n = 1}^{N} x_j + \mu^2
% \end{equation*}

% \begin{equation*}
% \frac{\partial lh}{\partial \sigma^2} = \sum_{n = 1}^{N} \Big[ \frac{\sigma^2}{2\sigma^4} - \frac{(x_j - \mu)^2}{2\sigma^4} \Big]
% \end{equation*}
%
% \begin{equation*}
% \frac{\partial lh}{\partial \sigma^2} = \sum_{n = 1}^{N} \Big[ \frac{\sigma^2 - (x_j - \mu)^2}{2\sigma^4} \Big]
% \end{equation*}
%
% \begin{equation*}
% \frac{\partial lh}{\partial \sigma^2} = \frac{1}{2\sigma^4}\sum_{n = 1}^{N} \Big[ \sigma^2 - (x_j - \mu)^2 \Big]
% \end{equation*}
%
% \begin{equation*}
% \frac{\partial lh}{\partial \sigma^2} = \frac{N\sigma^2}{2\sigma^4}\sum_{n = 1}^{N} \Big[ - (x_j - \mu)^2 \Big]
% \end{equation*}

% Para que $\frac{\partial lh}{\partial \sigma^2} = 0$, cada elemento del sumatorio tiene que ser $0$, i.e:
% \begin{equation*}
% \forall j \in \{1,\dots,N\},  \sigma^2 = (x_j - \mu)^2
% \end{equation*}

% \begin{equation*}
% \sigma =
% \begin{cases}
% x_j - \mu & \text{ si } x_j \geq \mu \\
% \mu - x_j & \text{ si } x_j < \mu \\
% \end{cases}
% \end{equation*}

La derivada de la función de máxima verosimilitud (likelihood) respecto de $\mu$ es:

\begin{equation*}
lh = \sum_{n = 1}^{N} \Big[ \ln{\big(  \sigma\sqrt{2\pi} \big)} + \frac{(x_j - \mu)^2}{2\sigma^2} \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \sum_{n = 1}^{N} \Big[ 0 + \frac{1}{2\sigma^2} \big(2(x_j - \mu)(-1)\big) \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \sum_{n = 1}^{N} \Big[-\frac{2(x_j - \mu)}{2\sigma^2}\Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \frac{1}{\sigma^2}\sum_{n = 1}^{N} \Big[-(x_j - \mu)\Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \frac{1}{\sigma^2}\sum_{n = 1}^{N} (\mu - x_j)
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \frac{N\mu}{\sigma^2} - \frac{1}{\sigma^2}\sum_{n = 1}^{N} x_j
\end{equation*}

Entonces para que $\frac{\partial lh}{\partial \mu} = 0$ se tiene que cumplir que

\begin{equation*}
\frac{N\mu}{\sigma^2} = \frac{1}{\sigma^2}\sum_{n = 1}^{N} x_j
\end{equation*}

\begin{equation*}
\frac{N\mu\sigma^2}{\sigma^2} = \sum_{n = 1}^{N} x_j
\end{equation*}

\begin{equation*}
N\mu = \sum_{n = 1}^{N} x_j
\end{equation*}

\begin{equation*}
\mu = \frac{1}{N}\sum_{n = 1}^{N} x_j
\end{equation*}
% ------------


Para tener un extremo deberían cumplirse estas condiciones:
\begin{equation*}
\sigma^2 = \frac{1}{N}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}
\begin{equation*}
\mu = \frac{1}{N}\sum_{n = 1}^{N} x_j
\end{equation*}

Que son las fórmulas clásicas para medir la media muestral y la varianza.

\section{Demostreu que realment són mínims (i no extrems qualsevol)}

Para demostrar que realmente son mínimos y no máximos hay que fijarse en la segunda derivada. Como se ha explicado anteriormente, se tratará de mínimos si la imagen de las dos segundas derivadas en esos puntos es positiva, y se tratará de máximos si son negativas.

Las derivadas son:

\begin{equation*}
lh = \sum_{n = 1}^{N} \Big[ \ln{\big(  \sigma\sqrt{2\pi} \big)} + \frac{(x_j - \mu)^2}{2\sigma^2} \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \sigma^2} = \frac{N}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

\begin{equation*}
\frac{\partial^2 lh}{\partial \sigma^4} = \Big[ \frac{N}{2}(-\frac{1}{\sigma^4})\Big] - \Big[\frac{1}{2}\sum_{n = 1}^{N} (x_j - \mu)^2(-\frac{2}{\sigma^6}) \Big]
\end{equation*}

\begin{equation*}
\frac{\partial^2 lh}{\partial \sigma^4} =  -\frac{N}{2\sigma^4} + \frac{1}{\sigma^6}\sum_{n = 1}^{N} (x_j - \mu)^2
\end{equation*}

Evaluamos en qué casos es mayor que $0$:

\begin{equation*}
-\frac{N}{2\sigma^4} + \frac{1}{\sigma^6}\sum_{n = 1}^{N} (x_j - \mu)^2 > 0
\end{equation*}

\begin{equation*}
\frac{1}{\sigma^6}\sum_{n = 1}^{N} (x_j - \mu)^2 > \frac{N}{2\sigma^4}
\end{equation*}

Puesto que $\sigma^6$ y $N$ son positivos podemos hacer:

\begin{equation*}
\frac{1}{N}\sum_{n = 1}^{N} (x_j - \mu)^2 > \frac{\sigma^6}{\sigma^4}
\end{equation*}

\begin{equation*}
\frac{2}{N}\sum_{n = 1}^{N} (x_j - \mu)^2 > \sigma^2
\end{equation*}

Y puesto que lo estamos evaluando en ese punto podemos hacer la sustitución:

\begin{equation*}
2\sigma^2 > \sigma^2
\end{equation*}

\begin{equation*}
2 > 1
\end{equation*}

Da valores positivos en todos los casos, por lo tanto realmente es un mínimo.

En el caso de $\mu$:

\begin{equation*}
lh = - \sum_{n = 1}^{N} \Big[ \ln{\big( f(x_n | \mu, \sigma^2)\big)} \Big]
\end{equation*}

\begin{equation*}
\frac{\partial lh}{\partial \mu} = \frac{N\mu}{\sigma^2} - \frac{1}{\sigma^2}\sum_{n = 1}^{N} x_j
\end{equation*}

\begin{equation*}
\frac{\partial^2 lh}{\partial \mu^2} = \frac{N}{\sigma^2} + 0
\end{equation*}

\begin{equation*}
\frac{\partial^2 lh}{\partial \mu^2} = \frac{N}{\sigma^2}
\end{equation*}

Puesto que N y $\sigma^2$ son positivos, la derivada segunda también lo es, y por lo tanto se trata de un mínimo.

\section{Calculeu els biaixos dels dos estimadors. Determineu si l'estimador per $\mu$ és consistent}

Todavía no lo tengo muy claro, pero por lo que he leído diría que $\tilde{\mu}$ no tiene bias, mientras que el bias de la varianza $\tilde{\sigma^2}$ es
\begin{equation*}
\frac{1}{N - 1}\tilde{\sigma^2}
\end{equation*}

A medida que $N \rightarrow \infty$, el bias se irá haciendo más pequeño. Por lo tanto podemos decir que el estimador para $\mu$  es consistente.


\section{Calculeu la variança de l'estimador per $\mu$ de 2 maneres (que han de coincidir), a triar de les 3 següents. Pista: useu que si $X_n,X_m \sim N(\mu,\sigma^2)$, llavors:}

\begin{equation*}
\mathbb{E} [ X_n \cdot X_m ] =
 \begin{cases}
      \mu^2 & \text{si } n \neq m \\
      \mu^2 + \sigma^2 & \text{si } n = m\\
   \end{cases}
\end{equation*}

\begin{enumerate}[(a)]
\item Insertant directament el valor de l'estimador i utilitzant les propietats de la variança;
\item Usant la coneguda fòrmula $Var[\hat{\theta}] = \mathbb{E}[\hat{\theta}] - (\mathbb{E}[\hat{\theta}])^2$;
\item Utilitzant la definició directa de la variança $Var[\hat{\theta}] = \mathbb{E}[(\mathbb{E}[\hat{\theta}] - \hat{\theta})^2]$
\end{enumerate}

\section{Per determinar la velocitat màxima d'un prototip d'avió es van fer 15 proves independents (i
caríssimes!) amb els resultats
\\
422.2, 418.7, 425.6, 420.3, 425.8, 423.1, 431.5, 428.2, 438.3, 434.0, 411.3, 417.2, 413.5, 441.3, 423.0
\\
on els valors venen expressats en m/s. Suposant que aquesta velocitat màxima és gaussiana, quins
valors estimaríeu per $\mu$ i per $\sigma$ ?}

\section{Fixeu unes $\mu$ i $\sigma^2$ al vostre gust i genereu 1000 mostres aleatòries simples Gaussianes de mida $N = 50$ (noteu que no cal emmagatzemar-les); utilitzeu les mostres (que han de ser i.i.d. i
independents entre si) per estimar els biaixos i variances dels valors teòrics
$\mu$ i $\sigma^2$. Compareu els
resultats respecte els valors teòrics dels estimadors.
}

\end{document}
