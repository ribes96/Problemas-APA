APA: Aprenentatge AutomÃ tic (TEMES 5 i 6)
Grau en Enginyeria InformÃ tica - UPC (2017/18)
LluÃ­s A. Belanche,

belanche@cs.upc.edu

Entrega: 11 Desembre 2017

Els problemes marcats

Objectius:

[G] sÃ³n de grup; els problemes/apartats marcats [R] sÃ³n per fer-se en R

1. ConÃ©ixer i saber derivar la funciÃ³ d'error mÃ©s adequada per classicaciÃ³
2. Saber crear i aplicar classicadors discriminatius Bayesians
3. Entendre i aplicar la regressiÃ³ logÃ­stica

Problema 1
Considerem un problema de classicaciÃ³ en dues classes, en les quals es disposa de les probabilitats de
cada classe P (C1 ) i P (C2 ). Considerem tres possibles regles per classicar un objecte:
1. (R1 ) Predir la classe mÃ©s probable
2. (R2 ) Predir la classe C1 amb probabilitat P (C1 )
3. (R3 ) Predir la classe C1 amb probabilitat 0.5
Es demana:
1. Donar les probabilitats d'error Pi (error) de les tres regles, i = 1, 2, 3
2. Demostrar que P1 (error) â‰¤ P2 (error) â‰¤ P3 (error)

.........
Problema 2 Funcions d'error per classicaciÃ³ [G]
L'objectiu dels models probabilÃ­stics discriminatius per classicaciÃ³ Ã©s modelar les probabilitats a posteriori P (Ck |x) per a cada classe k. En tasques de classicaciÃ³ binÃ ria (dues classes, C1 i C2 ), modelem
amb una funciÃ³ y(x) = P (C1 |x); llavors 1 âˆ’ y(x) = P (C2 |x). Tenim una mostra aleatÃ²ria simple D de
llargada N del mecanisme p(t, x), que escrivim D = {(x1 , t1 ), . . . , (xN , tN )}, on xn âˆˆ Rd i tn âˆˆ {0, 1}.
Prenem la convenciÃ³ que tn = 1 indica xn âˆˆ C1 i tn = 0 indica xn âˆˆ C2 , i modelem:

P (t|x) =

y(x)
1 âˆ’ y(x)

si xn âˆˆ C1
si xn âˆˆ C2

que pot ser mÃ©s convenientment expressat com P (t|x) = y(x)t (1 âˆ’ y(x))1âˆ’t , t = 0, 1. Aquesta Ã©s una
distribuciÃ³ de Bernoulli, la qual cosa permet d'obtenir una funciÃ³ d'error amb criteris ben fonamentats.
1. ConstruÃ¯u la funciÃ³ log-versemblanÃ§a de la mostra i proposeu una funciÃ³ d'error a partir d'ella.
2. Generalitzeu el resultat a un nÃºmero arbitrari K â‰¥ 2 de classes.

.........

TEMES 5 i 6

APA: Aprenentatge AutomÃ tic

Problema 3 Model probabilÃ­stic generatiu per variables binÃ ries
Considerem el cas de tenir d variables binÃ ries xi âˆˆ {0, 1} en un problema de classicaciÃ³ en K classes,
C1 , . . . , CK . La distribuciÃ³ conjunta P (x) = P (x1 , . . . , xd ) requereix en principi el coneixement de 2d âˆ’ 1
nÃºmeros (les respectives probabilitats de cada combinaciÃ³) per cada classe, la qual cosa no Ã©s factible.
Decidim doncs treballar amb distribucions condicionals (per cada classe k) de la forma:
P (x|Ck ) =

d
Y

pxkii (1 âˆ’ pki )1âˆ’xi

i=1

on pki Ã©s la probabilitat de tenir un 1 a la variable binÃ ria i per la classe k, que es pot estimar de les
dades. Es demana:
1. Argumenteu per quÃ¨ aquesta decissiÃ³ correspon a assumir que les d variables binÃ ries sÃ³n estadÃ­sticament independents donada la classe.
2. Doneu l'expressiÃ³ per les funcions discriminants ak (x) que en resulten. SÃ³n discriminants lineals?
3. Doneu l'expressiÃ³ per la probabilitat a posteriori P (Ck |x).

.........
Problema 4 LDA: model probabilÃ­stic generatiu per variables gaussianes [G]
Sabem que un vector aleatori continu en d variables X = (X1 , . . . , Xd )T segueix una distribuciÃ³ normal
(o gaussiana), cosa que escrivim X âˆ¼ N (Âµ, Î£), quan la seva densitat de probabilitat Ã©s:
p(x) =



1
T âˆ’1
(x
âˆ’
Âµ)
Î£
(x
âˆ’
Âµ)
exp
âˆ’
d
1
2
(2Ï€) 2 |Î£| 2
1

2
on Âµ Ã©s el vector de les mitjanes i Î£dÃ—d = (Ïƒij
) Ã©s la matriu de covariances. Volem disenyar un classicador probabilÃ­stic generatiu per un problema de dues classes (K = 2), on les distribucions condicionals
(per cada classe k) sÃ³n gaussianes amb
matriu de covarianÃ§a, o sigui X|Ck âˆ¼ N (Âµk , Î£).

igual

1. Mostreu que les probabilitats a posteriori es poden expressar P (Ck |x) = g(wT x + w0 ), on g Ã©s la
funciÃ³ logÃ­stica.
2. Doneu (calculant-los amb tots els passos) els valors per w i w0 . Argumenteu si obteniu un classicador lineal o no i per quÃ¨.
3. Exteneu el resultat al cas d'un nÃºmero arbitrari de classes K â‰¥ 2.

.........
Problema 5 La fÃ brica de pÃ­ndoles I
La companyia farmacÃ¨utica Nice Pills ha construit una cinta transportadora que porta dues classes de
pÃ­ndoles (adequades per dos tipus de malalties diferents), que anomenem C1 i C2 . Aquestes pÃ­ndoles
surten en dos colors: {yellow, white}, que sÃ³n detectats per una cÃ mera. La companyia fabrica pÃ­ndoles
en proporcions P (C1 ) = 13 , P (C2 ) = 32 . Se'ns facilita tambÃ© informaciÃ³ sobre la distribuciÃ³ del color per
cada classe: P (yellow|C1 ) = 51 , P (white|C1 ) = 54 i P (yellow|C2 ) = 23 , P (white|C2 ) = 31 . Es demana:
1. Quina Ã©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular les probabilitats P (yellow) i P (white) i les probabilitats P (C1 |yellow), P (C2 |yellow),
P (C1 |white) i P (C2 |white).
2

TEMES 5 i 6

APA: Aprenentatge AutomÃ tic

3. Quina Ã©s la decissiÃ³ Ã²ptima per pastilles yellow? I per pastilles white? Quins sÃ³n els
ambdÃ³s casos?

odds

en

4. Quina Ã©s la probabilitat d'error si s'utilitza el color per classicar? Per quÃ¨ Ã©s millor que la de
l'apartat 1?
Feu tots els cÃ lculs i doneu tots els resultats en forma de

.........

fraccions.

Problema 6 La fÃ brica de pÃ­ndoles II
La companyia farmacÃ¨utica Good Pills (competidora de l'anterior) ha construit una cinta transportadora
que porta dues classes de pÃ­ndoles (adequades per dos tipus de malalties diferents), que anomenem C1 i
C2 . Aquestes pÃ­ndoles surten en tres colors: {yellow, white, red}, que sÃ³n detectats per una cÃ mera. La
companyia fabrica pÃ­ndoles en proporcions P (C1 ) = 31 , P (C2 ) = 23 . Se'ns facilita tambÃ© informaciÃ³ sobre
la distribuciÃ³ del color per cada classe:
C1
C2

yellow
1/5
2/4

white
3/5
1/4

red
1/5
1/4

1. Quina Ã©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular les probabilitats P (yellow), P (white) i P (red) i les probabilitats P (C1 |yellow),P (C2 |yellow),
P (C1 |white),P (C2 |white),P (C1 |red) i P (C2 |red).
3. Quina Ã©s la decissiÃ³ Ã²ptima per pastilles yellow? I per pastilles white? I per pastilles red? Quins
sÃ³n els odds en tots els casos?
4. Quina Ã©s la probabilitat d'error si s'utilitza el color per classicar? Per quÃ¨ Ã©s millor que la de
l'apartat 1?
Feu tots els cÃ lculs i doneu tots els resultats en forma de

.........

fraccions.

Problema 7 La fÃ brica de pÃ­ndoles III [G]
La companyia farmacÃ¨utica Smart Pills (competidora de les anteriors) ha construit una cinta transportadora que porta dues classes de pÃ­ndoles (adequades per dos tipus de malalties diferents), que anomenem
C1 i C2 . Aquestes pÃ­ndoles surten en un ombrejat de colors que va del yellow al white (que Ã©s detectat per una cÃ mera, donant un valor continu en [0, 2]). La companyia fabrica pÃ­ndoles en proporcions
P (C1 ) = 13 , P (C2 ) = 32 . Se'ns facilita tambÃ© informaciÃ³ sobre la distribuciÃ³ (contÃ­nua) del color per cada
classe:
p(x|C1 ) =

2âˆ’x
x
, p(x|C2 ) =
2
2

1. Quina Ã©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular la distribuciÃ³

incondicional

del color p(x) = P (C1 )p(x|C1 ) + P (C2 )p(x|C2 ).

3. Calcular les distribucions de probabilitat P (C1 |x) i P (C2 |x).
4. Quina Ã©s la classicaciÃ³ Ã²ptima en funciÃ³ del color?
5. Quina Ã©s la probabilitat d'error si s'utilitza el color per classicar? Per quÃ¨ Ã©s millor que la de
l'apartat 1?

.........
3

TEMES 5 i 6

APA: Aprenentatge AutomÃ tic

Problema 8 Els classicadors LDA i QDA [G]
Sabem que un vector aleatori continu en d variables X = (X1 , . . . , Xd )T segueix una distribuciÃ³ normal
(o gaussiana), cosa que escrivim X âˆ¼ N (Âµ, Î£), quan la seva densitat de probabilitat Ã©s:



1
T âˆ’1
âˆ’ (x âˆ’ Âµ) Î£ (x âˆ’ Âµ)
p(x) =
d
1 exp
2
(2Ï€) 2 |Î£| 2
1

2
on Âµ Ã©s el vector de les mitjanes i Î£dÃ—d = (Ïƒij
) Ã©s la matriu de covariances. Volem disenyar un classicador probabilÃ­stic generatiu per un problema de dues classes (K = 2), on les distribucions condicionals
(per cada classe k) sÃ³n gaussianes, Ã©s a dir, X|Ck âˆ¼ N (Âµk , Î£k ). Sabem que el classicador de mÃ­nim risc
(anomenat regla de Bayes ) que minimitza la probabilitat d'error s'obtÃ© amb la fÃ²rmula de Bayes:

P (Ck |x) =

p(x|Ck )P (Ck )
p(x|Ck )P (Ck )
=
p(x)
p(x|C1 )P (C1 ) + p(x|C2 )P (C2 )

triant-se la classe Ck , k = 1, 2 que maximitza aquestes probabilitats a posteriori. Es demana:
1. ConstruÃ¯u la funciÃ³ discriminant per la classe Ck com gk (x) = ln {P (Ck )p(x|Ck )}; elimineu termes
que no afectin el resultat. Argumenteu quin tipus de superfÃ­cies de separaciÃ³ en resulten.

igual

2. Assumim ara que totes les classes tÃ©nen
matriu de covarianÃ§a, o sigui X|Ck âˆ¼ N (Âµk , Î£).
Simpliqueu l'expressiÃ³ anterior al mÃ xim. Argumenteu quin tipus de superfÃ­cies de separaciÃ³ en
resulten. Pel cas K = 2, Ã©s usual construir un Ãºnic discriminant g(x) = g1 (x) âˆ’ g2 (x) (sovint
anomenat un dichotomizer ). Expresseu-lo.
3. Veiem-ne un petit exemple numÃ¨ric per d = 3. Suposem les densitats gaussianes de classe:
Âµ1 = (0, 0, 0)T , Âµ2 = (1, 1, 1)T , Î£1 = Î£2 = diag

ConstruÃ¯u el

dichotomizer

1 1 1
, ,
, P (C2 ) = 2P (C1 )
4 4 4

i apliqueu-lo a la predicciÃ³ de l'exemple de test xâˆ— = (0.1, 0.7, 0.8)T

.........
Problema 9
Considerem dues distribucions condicionals (per cada classe) sÃ³n gaussianes bivariades (d = 2) amb
matriu de covarianÃ§a, de la forma X|Ck âˆ¼ N (Âµk , Î£), k = 1, 2.

igual

1. Suposant que les dues classes sÃ³n igual de probables, calculeu la regla de classicaciÃ³ Ã²ptima.
0.3
2. Apliqueu el resultat a les dades Âµ1 = (0, 0)T , Âµ2 = (3, 3)T i Î£ = ( 1.1
0.3 1.9 ), per obtenir una regla de
classicaciÃ³ concreta.

3. Classiqueu el punt xâˆ— = (1.0, 2.2)T .
4. Calculeu kxâˆ— âˆ’ Âµ1 k i kxâˆ— âˆ’ Âµ2 k i notareu que kxâˆ— âˆ’ Âµ1 k > kxâˆ— âˆ’ Âµ2 k. Com quadra aixÃ² amb el
resultat del punt anterior?

.........

4

TEMES 5 i 6

APA: Aprenentatge AutomÃ tic

Problema 10 Juguem a tennis?
Dos amics han recopilat dades sobre diverses vegades en que havien quedat per jugar a tennis (unes
vegades van acabar jugant i altres no, depenent de les previsions meteorolÃ²giques).
Outlook
Sunny
Sunny
Overcast
Rain
Rain
Rain
Overcast
Sunny
Sunny
Rain
Sunny
Overcast
Overcast
Rain

Temperature
Hot
Hot
Hot
Mild
Cool
Cool
Cool
Mild
Cool
Mild
Mild
Mild
Hot
Mild

Humidity
High
High
High
High
Normal
Normal
Normal
High
Normal
Normal
Normal
High
Normal
High

Wind
Weak
Strong
Weak
Weak
Weak
Strong
Strong
Weak
Weak
Weak
Strong
Strong
Weak
Strong

PlayTennis?
No
No
Yes
Yes
Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes
No

ConstruÃ¯u un classicador NaÃ¯ve Bayes i utilitzeu-lo per determinar si haurien de jugar a tenis en les
condicions d'un exemple de test xâˆ— = (Sunny, Hot, Normal, Weak)T . Noteu que no cal calcular totes les
probabilitats possibles, sinÃ³ nomÃ©s les imprescindibles per aquesta predicciÃ³ concreta.

.........
Problema 11 InterpretaciÃ³ de models de regressiÃ³ logÃ­stica
Considerem un model de regressiÃ³ logÃ­stica y(x) = g(wT x + w0 ), on g Ã©s la funciÃ³ logÃ­stica. Es demana:
1. Deriveu una interpretaciÃ³ per un coecient qualsevol wi (diferent de w0 ) a partir de la variaciÃ³ dels
odds quan xi passa a ser xi + Î´i i apliqueu-la al cas particular Î´i = 1.
2. Tenim y(x) = g(1.3x1 + 0.7x2 âˆ’ 0.29x3 + 0.54). Apliqueu la interpretaciÃ³ al coecient de x1 quan
Î´1 = 1 i al coecient de x3 quan Î´3 = âˆ’0.5.

.........
Problema 12 ObtenciÃ³ de la regressiÃ³ logÃ­stica
Una manera elegant d'arribar al model de regressiÃ³ logÃ­stica Ã©s partir dels odds. En tasques de classicaciÃ³
binÃ ria (dues classes, C1 i C2 ), considerem el logaritme natural dels odds (anomenat logit or log-odds )
per un x qualsevol:

ln

P (C1 |x)
P (C2 |x)




= ln

P (C1 |x)
1 âˆ’ P (C1 |x)



Resoleu aquesta fÃ²rmula en la probabilitat, calculant la funciÃ³ inversa de la logit. Deniu el model
que en resulta com el de regressiÃ³ logÃ­stica i doneu-ne una interpretaciÃ³ en termes de linealitat del model.

.........

5

