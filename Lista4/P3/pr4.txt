APA: Aprenentatge Autom√†tic (TEMES 5 i 6)
Grau en Enginyeria Inform√†tica - UPC (2017/18)
Llu√≠s A. Belanche,

belanche@cs.upc.edu

Entrega: 11 Desembre 2017

Els problemes marcats

Objectius:

[G] s√≥n de grup; els problemes/apartats marcats [R] s√≥n per fer-se en R

1. Con√©ixer i saber derivar la funci√≥ d'error m√©s adequada per classicaci√≥
2. Saber crear i aplicar classicadors discriminatius Bayesians
3. Entendre i aplicar la regressi√≥ log√≠stica

Problema 1
Considerem un problema de classicaci√≥ en dues classes, en les quals es disposa de les probabilitats de
cada classe P (C1 ) i P (C2 ). Considerem tres possibles regles per classicar un objecte:
1. (R1 ) Predir la classe m√©s probable
2. (R2 ) Predir la classe C1 amb probabilitat P (C1 )
3. (R3 ) Predir la classe C1 amb probabilitat 0.5
Es demana:
1. Donar les probabilitats d'error Pi (error) de les tres regles, i = 1, 2, 3
2. Demostrar que P1 (error) ‚â§ P2 (error) ‚â§ P3 (error)

.........
Problema 2 Funcions d'error per classicaci√≥ [G]
L'objectiu dels models probabil√≠stics discriminatius per classicaci√≥ √©s modelar les probabilitats a posteriori P (Ck |x) per a cada classe k. En tasques de classicaci√≥ bin√†ria (dues classes, C1 i C2 ), modelem
amb una funci√≥ y(x) = P (C1 |x); llavors 1 ‚àí y(x) = P (C2 |x). Tenim una mostra aleat√≤ria simple D de
llargada N del mecanisme p(t, x), que escrivim D = {(x1 , t1 ), . . . , (xN , tN )}, on xn ‚àà Rd i tn ‚àà {0, 1}.
Prenem la convenci√≥ que tn = 1 indica xn ‚àà C1 i tn = 0 indica xn ‚àà C2 , i modelem:

P (t|x) =

y(x)
1 ‚àí y(x)

si xn ‚àà C1
si xn ‚àà C2

que pot ser m√©s convenientment expressat com P (t|x) = y(x)t (1 ‚àí y(x))1‚àít , t = 0, 1. Aquesta √©s una
distribuci√≥ de Bernoulli, la qual cosa permet d'obtenir una funci√≥ d'error amb criteris ben fonamentats.
1. Constru√Øu la funci√≥ log-versemblan√ßa de la mostra i proposeu una funci√≥ d'error a partir d'ella.
2. Generalitzeu el resultat a un n√∫mero arbitrari K ‚â• 2 de classes.

.........

TEMES 5 i 6

APA: Aprenentatge Autom√†tic

Problema 3 Model probabil√≠stic generatiu per variables bin√†ries
Considerem el cas de tenir d variables bin√†ries xi ‚àà {0, 1} en un problema de classicaci√≥ en K classes,
C1 , . . . , CK . La distribuci√≥ conjunta P (x) = P (x1 , . . . , xd ) requereix en principi el coneixement de 2d ‚àí 1
n√∫meros (les respectives probabilitats de cada combinaci√≥) per cada classe, la qual cosa no √©s factible.
Decidim doncs treballar amb distribucions condicionals (per cada classe k) de la forma:
P (x|Ck ) =

d
Y

pxkii (1 ‚àí pki )1‚àíxi

i=1

on pki √©s la probabilitat de tenir un 1 a la variable bin√†ria i per la classe k, que es pot estimar de les
dades. Es demana:
1. Argumenteu per qu√® aquesta decissi√≥ correspon a assumir que les d variables bin√†ries s√≥n estad√≠sticament independents donada la classe.
2. Doneu l'expressi√≥ per les funcions discriminants ak (x) que en resulten. S√≥n discriminants lineals?
3. Doneu l'expressi√≥ per la probabilitat a posteriori P (Ck |x).

.........
Problema 4 LDA: model probabil√≠stic generatiu per variables gaussianes [G]
Sabem que un vector aleatori continu en d variables X = (X1 , . . . , Xd )T segueix una distribuci√≥ normal
(o gaussiana), cosa que escrivim X ‚àº N (¬µ, Œ£), quan la seva densitat de probabilitat √©s:
p(x) =



1
T ‚àí1
(x
‚àí
¬µ)
Œ£
(x
‚àí
¬µ)
exp
‚àí
d
1
2
(2œÄ) 2 |Œ£| 2
1

2
on ¬µ √©s el vector de les mitjanes i Œ£d√ód = (œÉij
) √©s la matriu de covariances. Volem disenyar un classicador probabil√≠stic generatiu per un problema de dues classes (K = 2), on les distribucions condicionals
(per cada classe k) s√≥n gaussianes amb
matriu de covarian√ßa, o sigui X|Ck ‚àº N (¬µk , Œ£).

igual

1. Mostreu que les probabilitats a posteriori es poden expressar P (Ck |x) = g(wT x + w0 ), on g √©s la
funci√≥ log√≠stica.
2. Doneu (calculant-los amb tots els passos) els valors per w i w0 . Argumenteu si obteniu un classicador lineal o no i per qu√®.
3. Exteneu el resultat al cas d'un n√∫mero arbitrari de classes K ‚â• 2.

.........
Problema 5 La f√†brica de p√≠ndoles I
La companyia farmac√®utica Nice Pills ha construit una cinta transportadora que porta dues classes de
p√≠ndoles (adequades per dos tipus de malalties diferents), que anomenem C1 i C2 . Aquestes p√≠ndoles
surten en dos colors: {yellow, white}, que s√≥n detectats per una c√†mera. La companyia fabrica p√≠ndoles
en proporcions P (C1 ) = 13 , P (C2 ) = 32 . Se'ns facilita tamb√© informaci√≥ sobre la distribuci√≥ del color per
cada classe: P (yellow|C1 ) = 51 , P (white|C1 ) = 54 i P (yellow|C2 ) = 23 , P (white|C2 ) = 31 . Es demana:
1. Quina √©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular les probabilitats P (yellow) i P (white) i les probabilitats P (C1 |yellow), P (C2 |yellow),
P (C1 |white) i P (C2 |white).
2

TEMES 5 i 6

APA: Aprenentatge Autom√†tic

3. Quina √©s la decissi√≥ √≤ptima per pastilles yellow? I per pastilles white? Quins s√≥n els
ambd√≥s casos?

odds

en

4. Quina √©s la probabilitat d'error si s'utilitza el color per classicar? Per qu√® √©s millor que la de
l'apartat 1?
Feu tots els c√†lculs i doneu tots els resultats en forma de

.........

fraccions.

Problema 6 La f√†brica de p√≠ndoles II
La companyia farmac√®utica Good Pills (competidora de l'anterior) ha construit una cinta transportadora
que porta dues classes de p√≠ndoles (adequades per dos tipus de malalties diferents), que anomenem C1 i
C2 . Aquestes p√≠ndoles surten en tres colors: {yellow, white, red}, que s√≥n detectats per una c√†mera. La
companyia fabrica p√≠ndoles en proporcions P (C1 ) = 31 , P (C2 ) = 23 . Se'ns facilita tamb√© informaci√≥ sobre
la distribuci√≥ del color per cada classe:
C1
C2

yellow
1/5
2/4

white
3/5
1/4

red
1/5
1/4

1. Quina √©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular les probabilitats P (yellow), P (white) i P (red) i les probabilitats P (C1 |yellow),P (C2 |yellow),
P (C1 |white),P (C2 |white),P (C1 |red) i P (C2 |red).
3. Quina √©s la decissi√≥ √≤ptima per pastilles yellow? I per pastilles white? I per pastilles red? Quins
s√≥n els odds en tots els casos?
4. Quina √©s la probabilitat d'error si s'utilitza el color per classicar? Per qu√® √©s millor que la de
l'apartat 1?
Feu tots els c√†lculs i doneu tots els resultats en forma de

.........

fraccions.

Problema 7 La f√†brica de p√≠ndoles III [G]
La companyia farmac√®utica Smart Pills (competidora de les anteriors) ha construit una cinta transportadora que porta dues classes de p√≠ndoles (adequades per dos tipus de malalties diferents), que anomenem
C1 i C2 . Aquestes p√≠ndoles surten en un ombrejat de colors que va del yellow al white (que √©s detectat per una c√†mera, donant un valor continu en [0, 2]). La companyia fabrica p√≠ndoles en proporcions
P (C1 ) = 13 , P (C2 ) = 32 . Se'ns facilita tamb√© informaci√≥ sobre la distribuci√≥ (cont√≠nua) del color per cada
classe:
p(x|C1 ) =

2‚àíx
x
, p(x|C2 ) =
2
2

1. Quina √©s la probabilitat d'error si no s'utilitza el color per classicar?
2. Calcular la distribuci√≥

incondicional

del color p(x) = P (C1 )p(x|C1 ) + P (C2 )p(x|C2 ).

3. Calcular les distribucions de probabilitat P (C1 |x) i P (C2 |x).
4. Quina √©s la classicaci√≥ √≤ptima en funci√≥ del color?
5. Quina √©s la probabilitat d'error si s'utilitza el color per classicar? Per qu√® √©s millor que la de
l'apartat 1?

.........
3

TEMES 5 i 6

APA: Aprenentatge Autom√†tic

Problema 8 Els classicadors LDA i QDA [G]
Sabem que un vector aleatori continu en d variables X = (X1 , . . . , Xd )T segueix una distribuci√≥ normal
(o gaussiana), cosa que escrivim X ‚àº N (¬µ, Œ£), quan la seva densitat de probabilitat √©s:



1
T ‚àí1
‚àí (x ‚àí ¬µ) Œ£ (x ‚àí ¬µ)
p(x) =
d
1 exp
2
(2œÄ) 2 |Œ£| 2
1

2
on ¬µ √©s el vector de les mitjanes i Œ£d√ód = (œÉij
) √©s la matriu de covariances. Volem disenyar un classicador probabil√≠stic generatiu per un problema de dues classes (K = 2), on les distribucions condicionals
(per cada classe k) s√≥n gaussianes, √©s a dir, X|Ck ‚àº N (¬µk , Œ£k ). Sabem que el classicador de m√≠nim risc
(anomenat regla de Bayes ) que minimitza la probabilitat d'error s'obt√© amb la f√≤rmula de Bayes:

P (Ck |x) =

p(x|Ck )P (Ck )
p(x|Ck )P (Ck )
=
p(x)
p(x|C1 )P (C1 ) + p(x|C2 )P (C2 )

triant-se la classe Ck , k = 1, 2 que maximitza aquestes probabilitats a posteriori. Es demana:
1. Constru√Øu la funci√≥ discriminant per la classe Ck com gk (x) = ln {P (Ck )p(x|Ck )}; elimineu termes
que no afectin el resultat. Argumenteu quin tipus de superf√≠cies de separaci√≥ en resulten.

igual

2. Assumim ara que totes les classes t√©nen
matriu de covarian√ßa, o sigui X|Ck ‚àº N (¬µk , Œ£).
Simpliqueu l'expressi√≥ anterior al m√†xim. Argumenteu quin tipus de superf√≠cies de separaci√≥ en
resulten. Pel cas K = 2, √©s usual construir un √∫nic discriminant g(x) = g1 (x) ‚àí g2 (x) (sovint
anomenat un dichotomizer ). Expresseu-lo.
3. Veiem-ne un petit exemple num√®ric per d = 3. Suposem les densitats gaussianes de classe:
¬µ1 = (0, 0, 0)T , ¬µ2 = (1, 1, 1)T , Œ£1 = Œ£2 = diag

Constru√Øu el

dichotomizer

1 1 1
, ,
, P (C2 ) = 2P (C1 )
4 4 4

i apliqueu-lo a la predicci√≥ de l'exemple de test x‚àó = (0.1, 0.7, 0.8)T

.........
Problema 9
Considerem dues distribucions condicionals (per cada classe) s√≥n gaussianes bivariades (d = 2) amb
matriu de covarian√ßa, de la forma X|Ck ‚àº N (¬µk , Œ£), k = 1, 2.

igual

1. Suposant que les dues classes s√≥n igual de probables, calculeu la regla de classicaci√≥ √≤ptima.
0.3
2. Apliqueu el resultat a les dades ¬µ1 = (0, 0)T , ¬µ2 = (3, 3)T i Œ£ = ( 1.1
0.3 1.9 ), per obtenir una regla de
classicaci√≥ concreta.

3. Classiqueu el punt x‚àó = (1.0, 2.2)T .
4. Calculeu kx‚àó ‚àí ¬µ1 k i kx‚àó ‚àí ¬µ2 k i notareu que kx‚àó ‚àí ¬µ1 k > kx‚àó ‚àí ¬µ2 k. Com quadra aix√≤ amb el
resultat del punt anterior?

.........

4

TEMES 5 i 6

APA: Aprenentatge Autom√†tic

Problema 10 Juguem a tennis?
Dos amics han recopilat dades sobre diverses vegades en que havien quedat per jugar a tennis (unes
vegades van acabar jugant i altres no, depenent de les previsions meteorol√≤giques).
Outlook
Sunny
Sunny
Overcast
Rain
Rain
Rain
Overcast
Sunny
Sunny
Rain
Sunny
Overcast
Overcast
Rain

Temperature
Hot
Hot
Hot
Mild
Cool
Cool
Cool
Mild
Cool
Mild
Mild
Mild
Hot
Mild

Humidity
High
High
High
High
Normal
Normal
Normal
High
Normal
Normal
Normal
High
Normal
High

Wind
Weak
Strong
Weak
Weak
Weak
Strong
Strong
Weak
Weak
Weak
Strong
Strong
Weak
Strong

PlayTennis?
No
No
Yes
Yes
Yes
No
Yes
No
Yes
Yes
Yes
Yes
Yes
No

Constru√Øu un classicador Na√Øve Bayes i utilitzeu-lo per determinar si haurien de jugar a tenis en les
condicions d'un exemple de test x‚àó = (Sunny, Hot, Normal, Weak)T . Noteu que no cal calcular totes les
probabilitats possibles, sin√≥ nom√©s les imprescindibles per aquesta predicci√≥ concreta.

.........
Problema 11 Interpretaci√≥ de models de regressi√≥ log√≠stica
Considerem un model de regressi√≥ log√≠stica y(x) = g(wT x + w0 ), on g √©s la funci√≥ log√≠stica. Es demana:
1. Deriveu una interpretaci√≥ per un coecient qualsevol wi (diferent de w0 ) a partir de la variaci√≥ dels
odds quan xi passa a ser xi + Œ¥i i apliqueu-la al cas particular Œ¥i = 1.
2. Tenim y(x) = g(1.3x1 + 0.7x2 ‚àí 0.29x3 + 0.54). Apliqueu la interpretaci√≥ al coecient de x1 quan
Œ¥1 = 1 i al coecient de x3 quan Œ¥3 = ‚àí0.5.

.........
Problema 12 Obtenci√≥ de la regressi√≥ log√≠stica
Una manera elegant d'arribar al model de regressi√≥ log√≠stica √©s partir dels odds. En tasques de classicaci√≥
bin√†ria (dues classes, C1 i C2 ), considerem el logaritme natural dels odds (anomenat logit or log-odds )
per un x qualsevol:

ln

P (C1 |x)
P (C2 |x)




= ln

P (C1 |x)
1 ‚àí P (C1 |x)



Resoleu aquesta f√≤rmula en la probabilitat, calculant la funci√≥ inversa de la logit. Deniu el model
que en resulta com el de regressi√≥ log√≠stica i doneu-ne una interpretaci√≥ en termes de linealitat del model.

.........

5

